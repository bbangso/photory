{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0p6JShGUnpZ"
   },
   "source": [
    "# NarrativeKoGPT2 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JV7_Ye-1UuS2"
   },
   "source": [
    "## 1.Google Drive 연동\n",
    "- 모델 파일과 학습 데이터가 저장 되어있는 구글 드라이브의 디렉토리와 Colab을 연동.  \n",
    "- 좌측상단 메뉴에서 런타임-> 런타임 유형 변경 -> 하드웨어 가속기 -> GPU 선택 후 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18LqQI0SVNX9"
   },
   "source": [
    "### 1.1 GPU 연동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2666,
     "status": "ok",
     "timestamp": 1584698768724,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "CmKD5vgYUeTa",
    "outputId": "b0ad3aed-4402-47d1-baf0-fad7f14a6c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 11 17:31:31 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 457.09       Driver Version: 457.09       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   44C    P8     6W /  N/A |   5632MiB /  6144MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       984    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A      1256    C+G   ...ngSettingsExpansionUI.exe    N/A      |\n",
      "|    0   N/A  N/A      4620    C+G   ...es\\CiscoCollabHostCef.exe    N/A      |\n",
      "|    0   N/A  N/A      5064    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      6204    C+G   ...zf8qxf38zg5c\\SkypeApp.exe    N/A      |\n",
      "|    0   N/A  N/A      7296    C+G   ...w5n1h2txyewy\\SearchUI.exe    N/A      |\n",
      "|    0   N/A  N/A     10060    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     10248    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     12360    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     13032      C   ...ython\\Python37\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     13288    C+G   ...es.TextInput.InputApp.exe    N/A      |\n",
      "|    0   N/A  N/A     15868    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16096    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     16176    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vi2gIIroVXeS"
   },
   "source": [
    "### 1.2 Google Drive 연동\n",
    "아래 코드를 실행후 나오는 URL을 클릭하여 나오는 인증 코드 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kpCSwfFGkRx7"
   },
   "source": [
    "**Colab 디렉토리 아래 NarrativeKoGPT2 경로 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2185,
     "status": "ok",
     "timestamp": 1584690917331,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "a7arJ4k2XLG_",
    "outputId": "b27dd97a-5cd5-4c7a-f9f4-c9fc5ec61476"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVmhNd21kse2"
   },
   "source": [
    "**필요 패키지들 설치**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20771,
     "status": "ok",
     "timestamp": 1584698816577,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "FDrIL81uXPB0",
    "outputId": "adbcd3a7-235c-456f-ece6-04b4aa5f00b1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1584698825809,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "vSCmVmaTlZ5S",
    "outputId": "0337cecb-5240-4c4e-8b6c-42114cb45d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\multicampus\\ssafy_pjt\\TextGeneration\\narrativeKoGPT2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hnSOCChk9lU"
   },
   "source": [
    "## 2.KoGPT2 Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL6xVLtHn6vK"
   },
   "source": [
    "### 2.1.Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13813,
     "status": "ok",
     "timestamp": 1584698842737,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "-IGI-Rcakhsw",
    "outputId": "43ac441c-c395-4de8-b814-af71aa13a731"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "from gluonnlp.data import SentencepieceTokenizer \n",
    "from kogpt2.utils import get_tokenizer\n",
    "from kogpt2.utils import download, tokenizer\n",
    "from model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
    "from util.data import NovelDataset\n",
    "import gluonnlp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01vGgfaNIDT_"
   },
   "source": [
    "**torch GPU 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1584688418157,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "ztdqTt3OIBPI",
    "outputId": "8991ed4a-104d-4ed5-f2bf-19a654fff0bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())  # GPU deviec count check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "import mxnet\n",
    "import transformers\n",
    "gluonnlp.__version__\n",
    "mxnet.__version__\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G20dHg4mn5x4"
   },
   "source": [
    "### 2.2. koGPT-2 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPoFzMKkk8eB"
   },
   "outputs": [],
   "source": [
    "ctx= 'cuda'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n",
    "cachedir='~/kogpt2/' # KoGPT-2 모델 다운로드 경로\n",
    "epoch =100  # 학습 epoch\n",
    "save_path = 'checkpoints/'\n",
    "#use_cuda = True # Colab내 GPU 사용을 위한 값\n",
    "\n",
    "pytorch_kogpt2 = {\n",
    "    'url':\n",
    "    'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
    "    'chksum': '676e9bcfa7'\n",
    "}\n",
    "kogpt2_config = {\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"layer_norm_epsilon\": 1e-05,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12,\n",
    "    \"n_positions\": 1024,\n",
    "    \"vocab_size\": 50000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xIXykCtn45d"
   },
   "source": [
    "### 2.3 Model and Vocab Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57506,
     "status": "ok",
     "timestamp": 1584698905026,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "kvtLJGh5o0MZ",
    "outputId": "5ee66192-e5df-47c9-b62f-366da32cca71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# download model\n",
    "model_info = pytorch_kogpt2\n",
    "model_path = download(model_info['url'],\n",
    "                       model_info['fname'],\n",
    "                       model_info['chksum'],\n",
    "                       cachedir=cachedir)\n",
    "# download vocab\n",
    "vocab_info = tokenizer\n",
    "vocab_path = download(vocab_info['url'],\n",
    "                       vocab_info['fname'],\n",
    "                       vocab_info['chksum'],\n",
    "                       cachedir=cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fu7-2csBpLQR"
   },
   "source": [
    "### 2.4.KoGPT-2 Model Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5AK_S6spqwQ"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT2LMHeadModel:\n\tsize mismatch for transformer.wpe.weight: copying a param with shape torch.Size([1024, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for transformer.h.0.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.1.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.2.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.3.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.4.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.5.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.6.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.7.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.8.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.9.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.10.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.11.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-a01b2b189485>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mkogpt2model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGPT2Config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkogpt2_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# model_path로부터 다운로드 받은 내용을 load_state_dict으로 업로드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mkogpt2model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\multicampus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 847\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    848\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT2LMHeadModel:\n\tsize mismatch for transformer.wpe.weight: copying a param with shape torch.Size([1024, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for transformer.h.0.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.1.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.2.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.3.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.4.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.5.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.6.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.7.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.8.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.9.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.10.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512]).\n\tsize mismatch for transformer.h.11.attn.bias: copying a param with shape torch.Size([1, 1, 1024, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512, 512])."
     ]
    }
   ],
   "source": [
    "# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
    "kogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
    "# model_path로부터 다운로드 받은 내용을 load_state_dict으로 업로드\n",
    "kogpt2model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device(ctx)\n",
    "kogpt2model.to(device)\n",
    "\n",
    "# kogpt2model.eval()\n",
    "# 추가로 학습하기 위해 .train() 사용\n",
    "kogpt2model.train()\n",
    "vocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
    "                                                     mask_token=None,\n",
    "                                                     sep_token=None,\n",
    "                                                     cls_token=None,\n",
    "                                                     unknown_token='<unk>',\n",
    "                                                     padding_token='<pad>',\n",
    "                                                     bos_token='<s>',\n",
    "                                                     eos_token='</s>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rnV010Wq9Xw"
   },
   "source": [
    "### 2.5. Get Batch Data using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2925,
     "status": "ok",
     "timestamp": 1584699184992,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "Ukfj9FPHpwfk",
    "outputId": "97d001d4-5220-4f26-fd12-c794b67c2510"
   },
   "outputs": [],
   "source": [
    "tok_path = get_tokenizer()\n",
    "model, vocab = kogpt2model, vocab_b_obj\n",
    "sentencepieceTokenizer = SentencepieceTokenizer(tok_path)\n",
    "\n",
    "#os.chdir(\"../\")\n",
    "data_file_path = 'Dataset/untokenized_Grimm.txt'\n",
    "batch_size = 1\n",
    "novel_dataset = NovelDataset(data_file_path, vocab, sentencepieceTokenizer)\n",
    "novel_data_loader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RFndCOIrLS0"
   },
   "source": [
    "### 2.6. Learning rate, Loss function, Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pY_o_C-qBhz"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMadZrwzXjbM"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "    \"\"\"Get the current gpu usage.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    usage: dict\n",
    "        Keys are device ids as integers.\n",
    "        Values are memory usage as integers in MB.\n",
    "    \"\"\"\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            'nvidia-smi', '--query-gpu=memory.used',\n",
    "            '--format=csv,nounits,noheader'\n",
    "        ], encoding='utf-8')\n",
    "    # Convert lines into a dictionary\n",
    "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
    "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
    "    return gpu_memory_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sgd28DRhthzo"
   },
   "source": [
    "### 2.7. KoGPT-2 Transfer Laerning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1584690255916,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "pbKCkcY63Y4a",
    "outputId": "f81332b5-4469-42e6-cb1c-da0324161bde"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1462,
     "status": "error",
     "timestamp": 1584759691454,
     "user": {
      "displayName": "김성환",
      "photoUrl": "",
      "userId": "17497395371430681608"
     },
     "user_tz": -540
    },
    "id": "yYkDU-cbrduY",
    "outputId": "ce32766a-431f-45fc-e784-518e670e7a0f"
   },
   "outputs": [],
   "source": [
    "print('KoGPT-2 Transfer Learning Start')\n",
    "epoch = 100\n",
    "for epoch in range(epoch):\n",
    "  count = 0\n",
    "  for data in novel_data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    data = torch.stack(data) # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n",
    "    \n",
    "    data= data.transpose(1,0)\n",
    "    data= data.to(ctx)\n",
    "    \n",
    "    outputs = model(data, labels=data)\n",
    "    loss, logits = outputs[:2]\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if count %10 ==0:\n",
    "      print('epoch no.{} train no.{}  loss = {}' . format(epoch, count+1, loss))\n",
    "      # torch.save(model,save_path+'checkpoint_{}_{}.tar'.format(epoch,count))\n",
    "      # 추론 및 학습 재개를 위한 일반 체크포인트 저장하기\n",
    "    if (count >0 and count%100==0) or epoch % 5 == 0:\n",
    "      torch.save({\n",
    "        'epoch': epoch,\n",
    "        'train_no': count,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss':loss\n",
    "      }, save_path+'narrativeKoGPT2_checkpoint.tar')\n",
    "      print('saved')\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by1zYlUWudTX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN2F5JshFIe4WPyUZzuAN8N",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NarrativeKoGPT2.ipynb",
   "provenance": [
    {
     "file_id": "1ChkS56shytnc2x8rXwaYwhhMJYPEQxaT",
     "timestamp": 1584444701945
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
